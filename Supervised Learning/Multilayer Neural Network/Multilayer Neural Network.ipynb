{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526345a4",
   "metadata": {},
   "source": [
    "# Multilayer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5de9ad",
   "metadata": {},
   "source": [
    "A multilayer neural network is multilayer of perceptrons. It consists an input layer, several hidden layers, output layer, fully connected weights and non-linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea86ef",
   "metadata": {},
   "source": [
    "## 1. Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251af5b",
   "metadata": {},
   "source": [
    "### Multilayer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6d6c5",
   "metadata": {},
   "source": [
    "First, a multilayer neural network has a non-linear activation function in most cases, and sigmoid ($\\sigma(\\cdot)$), hyperbolic tangent ($tanh(\\cdot)$) and rectified linear ($RELU(\\cdot)$) activation functions are widely used. $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "$$ tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}} $$$$ RELU(x) = \\begin{cases} x & \\text{if}\\ x > 0,\\\\0 & \\text{otherwise}\\end{cases} $$\n",
    "Further, a MLP consists of many layers (an input and an output layer with one or more hidden layers) of non-linear activating nodes. Since MLPs are fully connected, each node in one layer connects with a certain weight to every node in the following layer.\n",
    "\n",
    "Learning occurs in MLP by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is a supervised learning, and is carried out through backpropagation, a generalization of the least mean squares algorithm in the linear perceptron.\n",
    "\n",
    "The basic idea of backpropagation is to take advantage of the Chain Rule to derive derivative of each layers, from end to start.\n",
    "\n",
    "Error in an output node $j$ in the $n$ th training example by $e_j(n)=d_j(n)-y_j(n)$, where $d$ is the target value and $y$ is the value produced by the perceptron. The node weights can be adjusted by minimizing the sum square error in the entire output, given by\n",
    "\n",
    "$$\\mathcal{E}(n)=\\frac{1}{2}\\sum_j e_j^2(n)$$\n",
    "Using gradient descent, the change in each weight of output layer is\n",
    "\n",
    "$$\\Delta w_{ji} (n) = -\\gamma\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} y_i(n)$$\n",
    "where $y_i$ is the output of the previous neuron, $\\gamma$ is the learning rate, $v_j$ is the output of output layer.\n",
    "\n",
    "It is easy to prove that for an output node this derivative can be simplified to\n",
    "\n",
    "$$-\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} = e_j(n)\\phi^\\prime (v_j(n))$$\n",
    "where $\\phi^\\prime$ is the derivative of the activation function described above, which itself does not vary. For hidden nodes, it can be shown that the relevant derivative is\n",
    "\n",
    "$$-\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} = \\phi^\\prime (v_j(n))\\sum_k -\\frac{\\partial\\mathcal{E}(n)}{\\partial v_k(n)} w_{kj}(n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4950c",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d983c7",
   "metadata": {},
   "source": [
    "Batch gradient descent just take all the training data in every step of gradient descent, so it's just the basic form of gradient descent descussed in the Gradient Descent ssection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bc4a8",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ece92",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) considers one random sample in every step. As only one sample is used, the loss will not necessarily decrease. But the loss will decrease in a long training period. SGD generally converges faster when the dataset is large as it causes updates to the parameters more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a59fd8",
   "metadata": {},
   "source": [
    "## 2. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9256bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87da08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron():\n",
    "  \n",
    "    def __init__(self, layers = [784, 60, 60, 10], actFun_type='relu'):\n",
    "        self.actFun_type = actFun_type\n",
    "        self.layers = layers\n",
    "        self.L = len(self.layers)\n",
    "        self.W =[[0.0]]\n",
    "        self.B = [[0.0]]\n",
    "        for i in range(1, self.L):\n",
    "            w_temp = np.random.randn(self.layers[i], self.layers[i-1]) * np.sqrt(2/self.layers[i-1])\n",
    "            b_temp = np.random.randn(self.layers[i], 1) * np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "            self.W.append(w_temp)\n",
    "            self.B.append(b_temp)\n",
    "\n",
    "    def reset_weights(self, layers = [784, 60, 60, 10]):\n",
    "        self.layers = layers\n",
    "        self.L = len(self.layers)\n",
    "        self.W = [[0.0]]\n",
    "        self.B = [[0.0]]\n",
    "        for i in range(1, self.L):\n",
    "            w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "            b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "            self.W.append(w_temp)\n",
    "            self.B.append(b_temp)\n",
    "\n",
    "    def forward_pass(self, p, predict_vector = False):\n",
    "        Z =[[0.0]]\n",
    "        A = [p[0]]\n",
    "        for i in range(1, self.L):\n",
    "            z = (self.W[i] @ A[i-1]) + self.B[i]\n",
    "            a = self.actFun(z, self.actFun_type)\n",
    "            Z.append(z)\n",
    "            A.append(a)\n",
    "\n",
    "        if predict_vector == True:\n",
    "            return A[-1]\n",
    "        else:\n",
    "            return Z, A\n",
    "\n",
    "    def mse(self, a, y):\n",
    "        return .5*sum((a[i]-y[i])**2 for i in range(10))[0]\n",
    "\n",
    "    def MSE(self, data):\n",
    "        c = 0.0\n",
    "        for p in data:\n",
    "            a = self.forward_pass(p, predict_vector=True)\n",
    "            c += self.mse(a, p[1])\n",
    "        return c/len(data)\n",
    "\n",
    "    def actFun(self, z, type):\n",
    "        if type == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        elif type == 'sigmoid':\n",
    "            return 1.0 / (1.0 + np.exp(-z))\n",
    "        elif type == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def diff_actFun(self, z, type):\n",
    "        if type == 'tanh':\n",
    "            return 1.0 - (np.tanh(z))**2\n",
    "        elif type == 'sigmoid':\n",
    "            return self.actFun(z, type) * (1-self.actFun(z, type))\n",
    "        elif type == 'relu':\n",
    "            return np.where(z > 0, 1.0, 0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def deltas_dict(self, p):\n",
    "        Z, A = self.forward_pass(p)\n",
    "        deltas = dict()\n",
    "        deltas[self.L-1] = (A[-1] - p[1])*self.diff_actFun(Z[-1], self.actFun_type)\n",
    "        for l in range(self.L-2, 0, -1):\n",
    "            deltas[l] = (self.W[l+1].T @ deltas[l+1]) *self.diff_actFun(Z[l], self.actFun_type)\n",
    "\n",
    "        return A, deltas\n",
    "\n",
    "    def stochastic_gradient_descent(self, data, alpha = 0.04, epochs = 3):\n",
    "        print(\"Initial Cost = {}\".format(self.MSE(data)))\n",
    "        for k in range(epochs):\n",
    "            for p in data:\n",
    "                A, deltas = self.deltas_dict(p)\n",
    "                for i in range(1, self.L):\n",
    "                    self.W[i] = self.W[i] - alpha*deltas[i]@A[i-1].T\n",
    "                    self.B[i] = self.B[i] - alpha*deltas[i]\n",
    "        print('{0},{1},{2}'.format(k,'cost=',self.MSE(data)))\n",
    "\n",
    "\n",
    "    def mini_batch_gradient_descent(self, data, batch_size = 15, alpha = 0.04, epochs = 3):\n",
    "        print(\"Initial Cost = {}\".format(self.MSE(data)))\n",
    "        data_length = len(data)\n",
    "        for k in range(epochs):\n",
    "            for j in range(0, data_length-batch_size, batch_size):\n",
    "                delta_list = []\n",
    "                A_list = []\n",
    "                for p in data[j:j+batch_size]:\n",
    "                    A, deltas = self.deltas_dict(p)\n",
    "                    delta_list.append(deltas)\n",
    "                    A_list.append(A)\n",
    "\n",
    "                for i in range(1, self.L):\n",
    "                    self.W[i] = self.W[i] - (alpha/batch_size)*sum(da[0][i]@da[1][i-1].T for da in zip(delta_list, A_list))\n",
    "                    self.B[i] = self.B[i] - (alpha/batch_size)*sum(deltas[i] for deltas in delta_list)\n",
    "            print('{0},{1},{2}'.format(k,'cost=',self.MSE(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a833420",
   "metadata": {},
   "source": [
    "## 3. Applications on data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e0419",
   "metadata": {},
   "source": [
    "Fashion MNIST data set\n",
    "\n",
    "The Fashion MNIST data set is from keras. The dataset has 60,000 training data with 28x28 grayscale images and 10,000 test images. Images are labeled over 10 categories. The pixel depth allows 255 different intensities, with 0 being black and 255 being white. And the classes are: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot with labels 0 to 9 respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a845a2",
   "metadata": {},
   "source": [
    "Load data set. and split the dataset into train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51ae53e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 4us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 15s 1us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548d8a7",
   "metadata": {},
   "source": [
    "Visualize the first image by putting the pixel together, and it is an Ankle boot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f78363b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53a967",
   "metadata": {},
   "source": [
    "Scale images to $[0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09d0f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X/255\n",
    "test_X = test_X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3e5eb",
   "metadata": {},
   "source": [
    "Flatten images from matrix to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c604c893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0].flatten().reshape(28*28, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425488eb",
   "metadata": {},
   "source": [
    "Now we are ready to flatten the X matrix. Make a list first and then store the flatten vetors into it. And we also make the y to one hot encoded label vectors; we make a list and store the vectors into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28145434",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for x in train_X:\n",
    "    X.append(x.flatten().reshape(28*28, 1))\n",
    "\n",
    "# Y will temp store one-hot encoded label vectors\n",
    "Y = []\n",
    "for y in train_y:\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    Y.append(temp_vec)\n",
    "\n",
    "# Our data will be stored as a list of tuples. \n",
    "train_data = [p for p in zip(X, Y)]\n",
    "\n",
    "# the same method to deal with test data\n",
    "X = []\n",
    "for x in test_X:\n",
    "  X.append(x.flatten().reshape(784, 1))\n",
    "\n",
    "Y = []\n",
    "for y in test_y:\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    Y.append(temp_vec)\n",
    "\n",
    "test_data = [p for p in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ccf16",
   "metadata": {},
   "source": [
    "Now let's train MLP's using sigmoid, hyperbolic tangent, and rectified linear activation functions by mini batch gradient descent, and compare their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf3a8a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.8186099879304232\n",
      "0,cost=,0.17496766669041433\n",
      "1,cost=,0.15447443161922786\n",
      "2,cost=,0.14390902188288393\n",
      "3,cost=,0.1368800556914998\n",
      "4,cost=,0.13158901723811592\n"
     ]
    }
   ],
   "source": [
    "net_tanh = MultilayerPerceptron(layers=[784, 60, 60, 10], actFun_type='tanh')\n",
    "net_tanh.mini_batch_gradient_descent(train_data, batch_size = 16, alpha = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b4dfa24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13900711383494496"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_tanh.MSE(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eab4ac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 0.6492143747437975\n",
      "0,cost=,0.2513343006324802\n",
      "1,cost=,0.23909405254875646\n",
      "2,cost=,0.23300313583815424\n",
      "3,cost=,0.22862753941621047\n",
      "4,cost=,0.22582816918154586\n"
     ]
    }
   ],
   "source": [
    "net_relu = MultilayerPerceptron(layers=[784, 100, 100, 10], actFun_type='relu')\n",
    "net_relu.mini_batch_gradient_descent(train_data, batch_size = 16, alpha = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8829304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23400720081777213"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_relu.MSE(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af1e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.478722720766633\n",
      "0,cost=,0.4349701868948273\n",
      "1,cost=,0.40595944945621815\n",
      "2,cost=,0.3650028163970172\n",
      "3,cost=,0.3298019630822553\n"
     ]
    }
   ],
   "source": [
    "net_sig = MultilayerPerceptron(layers=[784, 100, 100, 10], actFun_type='sigmoid')\n",
    "net_sig.mini_batch_gradient_descent(train_data, batch_size = 16, alpha = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sig.MSE(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b96a30",
   "metadata": {},
   "source": [
    "Comparing three kinds of activation functions while keeping all other hyper-parameters unchanged, sigmoid activation function has the best perfomance on the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3_5",
   "language": "python",
   "name": "python3_5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
